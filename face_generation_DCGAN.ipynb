{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7497c919",
   "metadata": {},
   "source": [
    "\n",
    "# üé≠ Face Generation with DCGANs (Clean, Reproducible, GitHub-Ready)\n",
    "\n",
    "This notebook implements a **Deep Convolutional Generative Adversarial Network (DCGAN)** in TensorFlow/Keras to generate human face images using the **CelebA** dataset.  \n",
    "It includes:\n",
    "- Reproducible seeding\n",
    "- Clean `tf.data` input pipeline (finite cardinality; no re-batching warnings)\n",
    "- DCGAN architectures with recommended initializers\n",
    "- Custom training loop (`tf.keras.Model`)\n",
    "- Image grid callback per epoch\n",
    "- Model checkpointing\n",
    "- Loss curves\n",
    "- Optional mixed precision\n",
    "- Fully self-contained **Kaggle download** steps\n",
    "\n",
    "> Tip: Clear all outputs before committing to GitHub to keep the notebook size < 1‚ÄØMB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059548e",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Environment & Kaggle Setup\n",
    "\n",
    "Run this section once per new environment (e.g., Colab).  \n",
    "You will need your **`kaggle.json`** API key (from https://www.kaggle.com/settings/account).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab, you can quickly check for GPU:\n",
    "# !nvidia-smi || echo \"No GPU found.\"\n",
    "# Install required packages (lightweight)\n",
    "!pip -q install kaggle tensorflow numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, pathlib\n",
    "\n",
    "# Create Kaggle directory and place kaggle.json if not present\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "kaggle_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "if not os.path.exists(kaggle_path):\n",
    "    print(\"‚û°Ô∏è Please upload your kaggle.json in the next cell if not already present at ~/.kaggle/kaggle.json\")\n",
    "else:\n",
    "    print(\"‚úÖ Found kaggle.json at ~/.kaggle/kaggle.json\")\n",
    "    os.chmod(kaggle_path, 0o600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04261f0e",
   "metadata": {},
   "source": [
    "\n",
    "If you **don't** have `~/.kaggle/kaggle.json` already, run the next cell to upload it now (only needed on Colab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54758ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Colab-only: uncomment to use the file picker to upload kaggle.json\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # select kaggle.json\n",
    "# import shutil, os\n",
    "# if 'kaggle.json' in uploaded:\n",
    "#     dest = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "#     shutil.move('kaggle.json', dest)\n",
    "#     os.chmod(dest, 0o600)\n",
    "#     print('‚úÖ kaggle.json installed to', dest)\n",
    "# else:\n",
    "#     print('If running locally, place kaggle.json at ~/.kaggle/kaggle.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec569f64",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Download CelebA Dataset (Kaggle)\n",
    "\n",
    "We'll download **CelebA** via Kaggle. This dataset is large; ensure you have sufficient storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, subprocess, shlex, zipfile\n",
    "\n",
    "DATA_ROOT = \"/content/dataset\"  # change to a local path if not on Colab\n",
    "ZIP_PATH = os.path.join(DATA_ROOT, \"celeba-dataset.zip\")\n",
    "EXTRACT_DIR = os.path.join(DATA_ROOT, \"img_align_celeba\")\n",
    "\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    print(\">\", cmd)\n",
    "    p = subprocess.run(shlex.split(cmd), check=False, capture_output=True, text=True)\n",
    "    print(p.stdout or \"\")\n",
    "    if p.returncode != 0:\n",
    "        print(p.stderr or \"\")\n",
    "    return p.returncode == 0\n",
    "\n",
    "# Download only if not present\n",
    "if not os.path.exists(ZIP_PATH) and not os.path.isdir(EXTRACT_DIR):\n",
    "    print(\"‚¨áÔ∏è Downloading CelebA via Kaggle (jessicali9530/celeba-dataset)...\")\n",
    "    ok = run_cmd(f'kaggle datasets download -d jessicali9530/celeba-dataset -p {DATA_ROOT}')\n",
    "    if not ok:\n",
    "        print(\"‚ùå Kaggle download failed. Ensure kaggle.json is configured.\")\n",
    "\n",
    "# Unzip if not already extracted\n",
    "if os.path.exists(ZIP_PATH) and not os.path.isdir(EXTRACT_DIR):\n",
    "    print(\"üì¶ Extracting dataset... (this may take a few minutes)\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
    "        zf.extractall(DATA_ROOT)\n",
    "    print(\"‚úÖ Extraction complete.\")\n",
    "\n",
    "# Derive final images directory (inside the unzipped folder structure)\n",
    "IMG_DIR = None\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    candidate_dirs = [\n",
    "        EXTRACT_DIR,\n",
    "        os.path.join(EXTRACT_DIR, \"img_align_celeba\"),\n",
    "        os.path.join(DATA_ROOT, \"celeba-dataset\", \"img_align_celeba\"),\n",
    "    ]\n",
    "    for c in candidate_dirs:\n",
    "        if os.path.isdir(c) and len(os.listdir(c)) > 10000:  # celebA has many files\n",
    "            IMG_DIR = c\n",
    "            break\n",
    "\n",
    "if IMG_DIR is None:\n",
    "    # fall back to a reasonable default\n",
    "    fallback = os.path.join(DATA_ROOT, \"img_align_celeba\", \"img_align_celeba\")\n",
    "    if os.path.isdir(fallback):\n",
    "        IMG_DIR = fallback\n",
    "\n",
    "print(\"üóÇ IMG_DIR:\", IMG_DIR)\n",
    "assert IMG_DIR is not None, \"Could not locate the CelebA images directory after extraction.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f23fdd",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Reproducibility & Config\n",
    "Set a global seed and define training configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, numpy as np, tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "IM_SHAPE = (64, 64, 3)\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIM = 100\n",
    "EPOCHS = 50\n",
    "\n",
    "print(\"‚úÖ Seeds set. Config ‚Üí\", {\"IM_SHAPE\": IM_SHAPE, \"BATCH_SIZE\": BATCH_SIZE, \"LATENT_DIM\": LATENT_DIM, \"EPOCHS\": EPOCHS})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68f3bb",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) Mixed Precision\n",
    "Enable if your GPU supports it (saves memory & speeds up training). If unsure, skip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff9a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_MIXED_PRECISION = False  # set True if you know your GPU supports it well\n",
    "if USE_MIXED_PRECISION:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    print(\"‚úÖ Mixed precision enabled.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Mixed precision disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e847e",
   "metadata": {},
   "source": [
    "\n",
    "## 4) `tf.data` Pipeline (Finite, Shuffled, Prefetched)\n",
    "\n",
    "We create a finite dataset with shuffling and prefetching. Images are resized to 64√ó64 and normalized to **[-1, 1]** to match the generator's `tanh` output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a04e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "def preprocess(image):\n",
    "    image = tf.image.resize(image, (IM_SHAPE[0], IM_SHAPE[1]))\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "    return image\n",
    "\n",
    "# CelebA is in a flat directory; we use label_mode=None.\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    IMG_DIR,\n",
    "    label_mode=None,\n",
    "    image_size=(IM_SHAPE[0], IM_SHAPE[1]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ").map(preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83a978",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) Preview a Few Real Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9999e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "sample_batch = next(iter(train_dataset.take(1)))\n",
    "sample_imgs = (sample_batch + 1.0) / 2.0  # back to [0,1]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "n = 16\n",
    "for i in range(n):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(sample_imgs[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083dc288",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Models: Generator & Discriminator (DCGAN)\n",
    "\n",
    "We follow standard DCGAN design using transposed convolutions in the generator and strided convolutions in the discriminator. We use a **RandomNormal(0, 0.02)** initializer per DCGAN best practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "def build_generator(latent_dim=LATENT_DIM):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(8*8*256, use_bias=False, input_shape=(latent_dim,), kernel_initializer=init),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Reshape((8, 8, 256)),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(128, 5, strides=2, padding='same', use_bias=False, kernel_initializer=init),\n",
    "        tf.keras.layers.BatchNormalization(), tf.keras.layers.ReLU(),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False, kernel_initializer=init),\n",
    "        tf.keras.layers.BatchNormalization(), tf.keras.layers.ReLU(),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(3, 5, strides=2, padding='same', use_bias=False, activation='tanh', kernel_initializer=init)\n",
    "    ], name=\"generator\")\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_shape=IM_SHAPE):\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(64, 5, strides=2, padding='same', kernel_initializer=init, input_shape=input_shape),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        tf.keras.layers.Conv2D(128, 5, strides=2, padding='same', kernel_initializer=init),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1, kernel_initializer=init)\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers, name=\"discriminator_logits\")\n",
    "    # Wrap logits with a Sigmoid activation in float32 to avoid potential fp16 issues\n",
    "    inp = tf.keras.Input(shape=input_shape)\n",
    "    x = model(inp)\n",
    "    out = tf.keras.layers.Activation('sigmoid', dtype='float32', name=\"discriminator_output\")(x)\n",
    "    return tf.keras.Model(inp, out, name=\"discriminator\")\n",
    "\n",
    "generator = build_generator(LATENT_DIM)\n",
    "discriminator = build_discriminator(IM_SHAPE)\n",
    "generator.summary()\n",
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf90428",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Custom GAN (`tf.keras.Model`) with Training Step\n",
    "We implement the standard two-step GAN update: first train **D** on real+fake, then train **G** to fool **D**.  \n",
    "We use label smoothing (0.9 for real) to stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, generator, discriminator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.g_loss_tracker = tf.keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.d_loss_tracker = tf.keras.metrics.Mean(name=\"d_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.g_loss_tracker, self.d_loss_tracker]\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer, d_loss_fn, g_loss_fn=None):\n",
    "        super().compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        # same BCE for both by default\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn if g_loss_fn is not None else d_loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        generated_images = self.generator(random_latent_vectors, training=True)\n",
    "\n",
    "        # Label smoothing for real; no noise for simplicity\n",
    "        real_labels = tf.ones((batch_size, 1)) * 0.9\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_real = self.discriminator(real_images, training=True)\n",
    "            pred_fake = self.discriminator(generated_images, training=True)\n",
    "            d_loss_real = self.d_loss_fn(real_labels, pred_real)\n",
    "            d_loss_fake = self.d_loss_fn(fake_labels, pred_fake)\n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2.0\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        misleading_labels = tf.ones((batch_size, 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_latent_vectors, training=True)\n",
    "            pred = self.discriminator(fake_images, training=True)\n",
    "            g_loss = self.g_loss_fn(misleading_labels, pred)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
    "\n",
    "        self.g_loss_tracker.update_state(g_loss)\n",
    "        self.d_loss_tracker.update_state(d_loss)\n",
    "        return {\"g_loss\": self.g_loss_tracker.result(), \"d_loss\": self.d_loss_tracker.result()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ed5d0",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Callbacks: Image Grid per Epoch & Model Checkpoints\n",
    "We save a fixed grid of generated faces every epoch to monitor progress, and checkpoint generator weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class ShowImage(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, latent_dim, num_images=16, outdir=\"generated\"):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_images = num_images\n",
    "        self.seed = tf.random.normal([num_images, latent_dim])\n",
    "        self.outdir = outdir\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        generated = self.model.generator(self.seed, training=False)\n",
    "        generated = (generated + 1.0) / 2.0  # [-1,1] -> [0,1]\n",
    "\n",
    "        plt.figure(figsize=(6,6))\n",
    "        for i in range(self.num_images):\n",
    "            plt.subplot(4,4,i+1)\n",
    "            plt.imshow(generated[i].numpy())\n",
    "            plt.axis(\"off\")\n",
    "        fname = os.path.join(self.outdir, f\"epoch_{epoch+1:03d}.png\")\n",
    "        plt.savefig(fname, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"checkpoints/generator_epoch{epoch:03d}.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\"\n",
    ")\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb97de",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Compile & Train\n",
    "Train with Adam optimizers and Binary Cross-Entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee54cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gan = GAN(generator, discriminator, LATENT_DIM)\n",
    "gan.compile(\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    d_loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    ")\n",
    "\n",
    "history = gan.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[ShowImage(LATENT_DIM), checkpoint_cb],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3b193",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Loss Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['g_loss'], label=\"Generator Loss\")\n",
    "plt.plot(history.history['d_loss'], label=\"Discriminator Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990d65c",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Inference: Generate Fresh Samples\n",
    "Use the trained generator to sample new faces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_samples = 25\n",
    "z = tf.random.normal([num_samples, LATENT_DIM])\n",
    "imgs = gan.generator(z, training=False)\n",
    "imgs = (imgs + 1.0) / 2.0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(imgs[i].numpy())\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b555bcf",
   "metadata": {},
   "source": [
    "\n",
    "## 11) (Optional) Save Final Generator\n",
    "Save weights for later inference, and a small snippet showing how to load and generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save final generator weights\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "gen_w_path = \"artifacts/generator_final.weights.h5\"\n",
    "gan.generator.save_weights(gen_w_path)\n",
    "print(\"‚úÖ Saved:\", gen_w_path)\n",
    "\n",
    "# Example: reload and generate\n",
    "gen2 = build_generator(LATENT_DIM)\n",
    "gen2.load_weights(gen_w_path)\n",
    "z = tf.random.normal([4, LATENT_DIM])\n",
    "out = gen2(z, training=False)\n",
    "print(\"Reloaded generator output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c99979",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Next Steps\n",
    "- Add **FID** evaluation (even on a subset) for quantitative tracking.\n",
    "- Try **WGAN-GP** for improved stability.\n",
    "- Progressive growing (start 32√ó32 ‚Üí 64√ó64 ‚Üí ‚Ä¶).\n",
    "- Add light data augmentations (e.g., flips, jitter).\n",
    "- Run for more epochs with mixed precision if supported.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
